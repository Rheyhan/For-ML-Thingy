{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: guykuo@carson.u.washington.edu (Guy Kuo)\n",
      "Subject: SI Clock Poll - Final Call\n",
      "Summary: Final call for SI clock reports\n",
      "Keywords: SI,acceleration,clock,upgrade\n",
      "Article-I.D.: shelley.1qvfo9INNc3s\n",
      "Organization: University of Washington\n",
      "Lines: 11\n",
      "NNTP-Posting-Host: carson.u.washington.edu\n",
      "\n",
      "A fair number of brave souls who upgraded their SI clock oscillator have\n",
      "shared their experiences for this poll. Please send a brief message detailing\n",
      "your experiences with the procedure. Top speed attained, CPU rated speed,\n",
      "add on cards and adapters, heat sinks, hour of usage per day, floppy disk\n",
      "functionality with 800 and 1.4 m floppies are especially requested.\n",
      "\n",
      "I will be summarizing in the next two days, so please add to the network\n",
      "knowledge base if you have done the clock upgrade and haven't answered this\n",
      "poll. Thanks.\n",
      "\n",
      "Guy Kuo <guykuo@u.washington.edu>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#You can just use sklearn to get dataset for sentences\n",
    "\n",
    "#libraries\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "#get model\n",
    "vectorizer= text.CountVectorizer()\n",
    "\n",
    "#fetching all the possible text data\n",
    "news=fetch_20newsgroups()\n",
    "\n",
    "print(news.data[1])         #print one of the many news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Love:   652\n",
      "from,56979  |Occurence: 22670\n",
      "lerxst,75358  |Occurence: 4\n",
      "wam,123162  |Occurence: 81\n",
      "umd,118280  |Occurence: 295\n",
      "edu,50527  |Occurence: 21321\n",
      "where,124031  |Occurence: 2716\n",
      "my,85354  |Occurence: 9703\n",
      "thing,114688  |Occurence: 1532\n",
      "subject,111322  |Occurence: 12264\n",
      "what,123984  |Occurence: 9861\n",
      "car,37780  |Occurence: 1311\n",
      "is,68532  |Occurence: 43480\n",
      "this,114731  |Occurence: 20121\n",
      "nntp,87620  |Occurence: 4814\n",
      "posting,95162  |Occurence: 5507\n",
      "host,64095  |Occurence: 4996\n",
      "rac3,98949  |Occurence: 7\n",
      "organization,90379  |Occurence: 11233\n",
      "university,118983  |Occurence: 5586\n",
      "of,89362  |Occurence: 69034\n",
      "maryland,79666  |Occurence: 127\n"
     ]
    }
   ],
   "source": [
    "#fitting model\n",
    "vectorizer.fit(news.data)\n",
    "\n",
    "#transform token arr\n",
    "token=vectorizer.transform(news.data)\n",
    "tokenarr=token.toarray()\n",
    "#find how many \"love\" are there\n",
    "print(f'Love:   {sum(tokenarr[:,vectorizer.vocabulary_[\"love\"]])}')\n",
    "\n",
    "#print 20 first word, index and its occurences from the entire data set\n",
    "n=20;i=0\n",
    "for word, index in vectorizer.vocabulary_.items():\n",
    "    if i>n:\n",
    "        break\n",
    "    print(f'{word},{index}  |Occurence: {sum(tokenarr[:,index])}')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "\n",
      "before: 130107 |     After:101631 \n",
      "Decrease: 28476\n"
     ]
    }
   ],
   "source": [
    "#Data cleaning\n",
    "'''\n",
    "machine learning purposes words like 'Subject', 'From', 'Organization', \n",
    "'Nntp-Posting-Host', 'Lines' and many others are useless, because\n",
    "they occur in all or in most postings. The technical 'garbage' from the\n",
    "newsgroup can be easily stripped off.\n",
    "'''\n",
    "print(news.data[0])\n",
    "\n",
    "#We'll delete the headers, footers and quotes\n",
    "cleanednews=fetch_20newsgroups(remove=('headers', 'footers', 'quotes'))\n",
    "print(cleanednews.data[0], end=\"\\n\\n\\n\")\n",
    "\n",
    "vectorizer2=text.CountVectorizer()\n",
    "vectorizer2.fit(cleanednews.data)\n",
    "\n",
    "#comparison len of words before and after\n",
    "print(f'before: {len(vectorizer.vocabulary_)} |     After:{len(vectorizer2.vocabulary_)} \\nDecrease: {len(vectorizer.vocabulary_)-len(vectorizer2.vocabulary_)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score  : 0.6496282527881041\n",
      "f1 scoreq       : 0.6235320303005205\n"
     ]
    }
   ],
   "source": [
    "#Example of being used in Machine learning\n",
    "\n",
    "#libraries\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB       #classification purpose\n",
    "import pandas as pd\n",
    "\n",
    "#get data\n",
    "dummytrain=fetch_20newsgroups(subset=\"train\", remove=('headers', 'footers', 'quotes'))\n",
    "dummytest=fetch_20newsgroups(subset=\"test\", remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "#preparing train data, label and test data and its label\n",
    "vectorizer3=text.CountVectorizer()\n",
    "traindata=(vectorizer3.fit_transform(dummytrain.data))\n",
    "trainlabel=dummytrain.target\n",
    "\n",
    "testdata=vectorizer3.transform(dummytest.data)  #we use transform cuz some words aren't present on dummytest.data\n",
    "testlabel=dummytest.target\n",
    "\n",
    "#model of the classifier and vectorizer\n",
    "thefunny=MultinomialNB(alpha=0.05)\n",
    "thefunny.fit(traindata,trainlabel)\n",
    "\n",
    "#predicted\n",
    "predictedlabel=thefunny.predict(testdata)\n",
    "\n",
    "#Score\n",
    "print(f'accuracy score  : {accuracy_score(testlabel, predictedlabel)}')\n",
    "print(f'f1 scoreq       : {f1_score(testlabel, predictedlabel, average=\"macro\")}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
